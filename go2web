#!/usr/bin/env python3
import os
import sys
import re
import socket
import json
import argparse
import ssl
import hashlib
import datetime
from urllib.parse import urlparse, urlencode
import webbrowser

from markdownify import markdownify
from bs4 import BeautifulSoup
from termcolor import cprint

from icecream import ic

CACHE_PATH = "./cache/"

class HTTPSocket:
    '''
    '''
    BLOCK_SZ = 4096

    def __init__(self, host, sock=None):
        self.host = host
        if sock is None:
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        else:
            self.sock = sock
        context = ssl.create_default_context()
        self.sock = context.wrap_socket(self.sock, server_hostname=host)

        self.sock.connect((host, 443))

    def _get_remaining_bytes(self, data, content_len):
        '''Once you have received all the headers,
        you will either know the length of the content by the Content-Length,
        or it will be chunked.
        This method takes the partial data received after the headers, and fetches the rest.'''
        while len(data) < content_len:
            sz = min(self.BLOCK_SZ, content_len - len(data))
            chunk = self.sock.recv(sz)
            if chunk == b'':
                raise RuntimeError("socket connection broken")
            data += chunk
        return data

    def _get_remaining_bytes_chunked(self, data):
        '''This method takes the partial data received after the headers,
        and fetches the rest, considering the content is chunked.'''
        content = b''

        # assuming we've recv'd at least the first chunk length already
        while True:
            crlf = data.find(b'\r\n')
            if crlf == -1:
                break

            num_str = data[:crlf]
            chunk_len = int(num_str, 16)
            data = data[crlf+2:]

            if chunk_len == 0:
                break

            while chunk_len+2+3 > len(data):  # 2 for \r\f, 3 for the potential \0\r\f
                chunk = self.sock.recv(self.BLOCK_SZ)
                if chunk == b'':
                    raise RuntimeError("socket connection broken")
                data += chunk

            content += data[:chunk_len]
            data = data[chunk_len+2:]
        return content

    def _retrieve_from_cache(self, url):
        path = self._get_url_cache_path(url)
        if not os.path.exists(path):
            return None

        # check if cache is too old
        ts = os.path.getmtime(path)
        last_mod_time = datetime.datetime.fromtimestamp(ts)
        time_diff = datetime.datetime.now() - last_mod_time
        if time_diff.days > 7:
            return None

        with open(path, 'r') as f:
            json_str = f.read()
        js_obj = json.JSONDecoder().decode(json_str)
        response, headers, status = js_obj['response'], js_obj['headers'], js_obj['status']
        return response, headers, status


    def _get_url_cache_path(self, url):
        fn = hashlib.sha256(url.encode()).hexdigest() + '.json'
        path = CACHE_PATH + fn
        return path

    def _store_in_cache(self, url, response, headers, status):
        if not os.path.exists(CACHE_PATH):
            os.makedirs(CACHE_PATH)

        obj = {"response": response, "headers": headers, "status": status}
        json_str = json.JSONEncoder().encode(obj)
        path = self._get_url_cache_path(url)
        with open(path, 'w') as f:
            f.write(json_str)
        return


    def request(self, url):
        if (ret := self._retrieve_from_cache(url)) is not None:
            return ret

        parsed_url = urlparse(url, scheme='http')
        path = parsed_url.path or '/'
        query_string = parsed_url.query

        request = f"GET {path}?{query_string} HTTP/1.1\r\nHost: {self.host}\r\n\r\n"
        self.sock.sendall(request.encode())

        data = b''
        bytes_recd = 0
        # retrieve status line

        # receive the entire headers
        while True:
            chunk = self.sock.recv(self.BLOCK_SZ)
            if chunk == b'':
                raise RuntimeError("socket connection broken")
            data += chunk
            bytes_recd = bytes_recd + len(chunk)
            if b'\r\n\r\n' in chunk:  # Check for end of headers
                break

        i = data.find(b'\r\n')
        status = data[:i].decode().split(maxsplit=2)
        data = data[i+2:]
        ic(status)

        end = data.find(b'\r\n\r\n')

        headers = {}
        for l in data[:end].split(b'\r\n'):
            k, *v = l.split(b": ", maxsplit=1)
            k = k.lower()
            v = v[0] if v else b""
            headers[k.decode()] = v.decode()

        ic(headers)
        data = data[end+4:]

        chunked = False
        content_len = int(headers.get('content-length', -1))
        if content_len == -1:
            if headers.get('transfer-encoding') != 'chunked':
                raise RuntimeError("No 'Content-Length' in the HTTP response, and not chunked.")
            chunked = True

        content = self._get_remaining_bytes_chunked(data) if chunked\
            else self._get_remaining_bytes(data, content_len)

        charset = 'utf-8'
        val = headers.get('content-type')
        if val and 'charset' in val:
            charset = val[val.find('charset=')+len('charset='):]
        response = content.decode(charset)

        self._store_in_cache(url, response, headers, status)

        return response, headers, status



def search(search_query, limit=10):
    host = 'search.marginalia.nu'
    s = HTTPSocket(host)
    query = {'query': search_query}
    resp, headers = s.request(f'https://search.marginalia.nu/search?{urlencode(query)}')
    soup = BeautifulSoup(resp, 'html.parser')

    cards = soup.find(id="results").find_all(class_="card")[:limit]
    results = [(c.find('h2').get_text().strip(), c.find('a').get('href')) for c in cards]

    for i, (title, href) in enumerate(results):
        cprint(f"{i+1}. ", "cyan", end="")
        cprint(title, "light_green")
        cprint(href, "dark_grey")
        print()

    chosen_id = int(input("Pick a link (e.g. 4): ")) - 1
    url = results[chosen_id][1]

    if input("Do you want to open it in your browser? [Y/n]: ") in ["", "Y", 'y']:
        webbrowser.open(url)
    else:
        browse(url)


def browse(url):
    if '://' not in url:
        url = 'https://' + url

    while True:
        ic(url)
        parsed_url = urlparse(url, scheme='http')
        host = parsed_url.hostname
        s = HTTPSocket(host)
        resp, headers, status = s.request(url)

        if status[1][0] == '3':
            url = headers['location']
        else:
            break

    if headers.get('content-type') == 'application/json':
        print(json.dumps(json.loads(resp), sort_keys=True, indent=4))
        return

    md_text = markdownify(resp).replace('\n\n', '\n')
    md_text = re.sub(r'\n\s*\n', '\n\n', md_text)  # join empty lines
    print(md_text)



if __name__ == '__main__':
    parser = argparse.ArgumentParser(
                    prog='go2web',
                    description='Access the web from your terminal.')
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--url', '-u', help="Print web page to the terminal")
    group.add_argument('--search', '-s', metavar='QUERY', help="Look up in a search engine")
    parser.add_argument('--debug', '-d', action="store_true", help="Enable debug output")

    args = parser.parse_args()

    if len(sys.argv)==1:
        parser.print_help()
        sys.exit()

    if not args.debug:
        ic.disable()

    if args.search:
        search(args.search)
        sys.exit()

    if args.url:
        browse(args.url)
        sys.exit()
