#!/usr/bin/env python3
import sys
import socket
import argparse
import ssl
from urllib.parse import urlparse, urlencode
import webbrowser

from bs4 import BeautifulSoup
from termcolor import cprint

from icecream import ic

class HTTPSocket:
    '''
    '''
    BLOCK_SZ = 4096

    def __init__(self, host, sock=None):
        self.host = host
        if sock is None:
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        else:
            self.sock = sock
        context = ssl.create_default_context()
        self.sock = context.wrap_socket(self.sock, server_hostname=host)

        self.sock.connect((host, 443))

    def _get_remaining_bytes(self, data, content_len):
        '''Once you have received all the headers,
        you will either know the length of the content by the Content-Length,
        or it will be chunked.
        This method takes the partial data received after the headers, and fetches the rest.'''
        while len(data) < content_len:
            sz = min(self.BLOCK_SZ, content_len - len(data))
            chunk = self.sock.recv(sz)
            if chunk == b'':
                raise RuntimeError("socket connection broken")
            data += chunk
        return data

    def _get_remaining_bytes_chunked(self, data):
        '''This method takes the partial data received after the headers,
        and fetches the rest, considering the content is chunked.'''
        content = b''

        # assuming we've recv'd at least the first chunk length already
        while True:
            crlf = data.find(b'\r\n')
            if crlf == -1:
                break

            num_str = data[:crlf]
            chunk_len = int(num_str, 16)
            data = data[crlf+2:]

            if chunk_len == 0:
                break

            while chunk_len+2+3 > len(data):  # 2 for \r\f, 3 for the potential \0\r\f
                chunk = self.sock.recv(self.BLOCK_SZ)
                if chunk == b'':
                    raise RuntimeError("socket connection broken")
                data += chunk

            content += data[:chunk_len]
            data = data[chunk_len+2:]
        return content

    def request(self, url):
        parsed_url = urlparse(url, scheme='http')
        path = parsed_url.path or '/'
        query_string = parsed_url.query

        request = f"GET {path}?{query_string} HTTP/1.1\r\nHost: {self.host}\r\n\r\n"
        self.sock.sendall(request.encode())

        data = b''
        bytes_recd = 0
        # receive the entire headers
        while True:
            chunk = self.sock.recv(self.BLOCK_SZ)
            if chunk == b'':
                raise RuntimeError("socket connection broken")
            data += chunk
            bytes_recd = bytes_recd + len(chunk)
            if b'\r\n\r\n' in chunk:  # Check for end of headers
                break

        end = data.find(b'\r\n\r\n')

        headers = {}
        for l in data[:end].split(b'\r\n'):
            k, *v = l.split(b": ", maxsplit=1)
            k = k.lower()
            v = v[0] if v else b""
            headers[k] = v

        ic(headers)
        data = data[end+4:]

        chunked = False
        content_len = int(headers.get(b'content-length', -1))
        if content_len == -1:
            if headers.get(b'transfer-encoding') != b'chunked':
                raise RuntimeError("No 'Content-Length' in the HTTP response, and not chunked.")
            chunked = True

        content = self._get_remaining_bytes_chunked(data) if chunked\
            else self._get_remaining_bytes(data, content_len)

        charset = 'utf-8'
        for name, val in headers.items():
            if name == b'content-type' and b'charset' in val:
                charset = val[val.find(b'charset=')+len('charset='):].decode()
                break
        return content.decode(charset)



def search(search_query, limit=10):
    host = 'search.marginalia.nu'
    s = HTTPSocket(host)
    query = {'query': search_query}
    resp = s.request(f'https://search.marginalia.nu/search?{urlencode(query)}')
    soup = BeautifulSoup(resp, 'html.parser')

    cards = soup.find(id="results").find_all(class_="card")[:limit]
    results = [(c.find('h2').get_text().strip(), c.find('a').get('href')) for c in cards]

    for i, (title, href) in enumerate(results):
        cprint(f"{i+1}. ", "cyan", end="")
        cprint(title, "light_green")
        cprint(href, "dark_grey")
        print()

    chosen_id = int(input("Pick a link (e.g. 4): ")) - 1
    url = results[chosen_id][1]

    if input("Do you want to open it in your browser? [Y/n]: ") in ["", "Y", 'y']:
        webbrowser.open(url)
    else:
        browse(url)


def browse(url):
    if '://' not in url:
        url = 'https://' + url

    parsed_url = urlparse(url, scheme='http')
    host = parsed_url.hostname
    s = HTTPSocket(host)
    resp = s.request(url)
    soup = BeautifulSoup(resp, 'html.parser')

    print(soup.get_text())


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
                    prog='go2web',
                    description='Access the web from your terminal.')
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--url', '-u', help="Print web page to the terminal")
    group.add_argument('--search', '-s', metavar='QUERY', help="Look up in a search engine")
    parser.add_argument('--debug', '-d', action="store_true", help="Enable debug output")

    args = parser.parse_args()

    if len(sys.argv)==1:
        parser.print_help()
        sys.exit()

    if not args.debug:
        ic.disable()

    if args.search:
        search(args.search)
        sys.exit()

    if args.url:
        browse(args.url)
        sys.exit()
